# Di√°rio de Bordo - 2026-02-11

## 17:35 - Diagn√≥stico: Modelos a Falhar
**Problema:** Carlos reportou que "os outros modelos est√£o todos a falhar"
**Causa:** Config tinha `fallbacks: []` (vazio)
**Solu√ß√£o:** Adicionei fallback chain via `gateway config.patch`

## 17:57 - Remo√ß√£o de Gemini 2.0
Removido google/gemini-2.0-flash-001 dos fallbacks (n√£o estava confi√°vel)

## 18:00 - Testes de Modelos
- Pony-Alpha: ‚ùå Falhou (sem output)
- Kimi 8k: ‚úÖ Operacional
- Kimi 32k: ‚úÖ Operacional
- Kimi 128k: ‚úÖ Operacional

## 18:30 - Reconfigura√ß√£o de Fallbacks
**Final chain:**
- Primary: anthropic/claude-haiku-4-5
- Fallback 1: moonshot/moonshot-v1-8k (Kimi)
- Fallback 2: ollama/qwen3-coder:30b (Local)

## 19:30 - Corre√ß√£o de 4 Crons
Todas tarefas agora usam ollama/qwen3-coder:30b (modelo anterior era google-antigravity/gemini-3-pro-low desativado)

## 20:30 - Context Files Auditoria
Ficheiros de contexto revistos e mantidos (conte√∫do apropriado para brain files)

## 21:00 - Integra√ß√£o Gemini 2.5 Flash
- ‚úÖ API key Google armazenada em tech/secrets/google_ai_key (chmod 600)
- ‚úÖ Provider google-ai configurado
- ‚úÖ Modelo gemini-2.5-flash adicionado (1M context window)
- ‚ö†Ô∏è URGENT: API key deve ser trocada (foi compartilhada em Telegram)

## 21:30 - Testes GLM
**Configura√ß√£o final:**
- Primary: ollama/glm-5:cloud (GLM vers√£o 5)
- Fallback 1: moonshot/moonshot-v1-8k (Kimi)
- Fallback 2: google-ai/gemini-2.5-flash
- Fallback 3: ollama/qwen3-coder:30b (Local)

**Modelos dispon√≠veis:**
1. GLM-5 (Alibaba/Local) - Primary
2. Kimi 8k, 32k, 128k (Moonshot/Cloud)
3. Gemini 2.5 Flash (Google AI/Cloud) üÜï
4. DeepSeek R1 Free (OpenRouter)
5. Qwen3-Coder 30B (Local)

## 21:35+ - Testes do GLM-5
**Primary alterado para:** ollama/glm-5:cloud (Alibaba)
**Testes:** GLM respondendo corretamente, em andamento

## 21:45+ - Limpeza de Modelos Locais
**Removidos:**
- glm-4.7-flash:latest e glm-4.7-flash-64k:latest (redundantes com GLM-5)
- deepseek-r1:32b-64k e deepseek-r1:32b (n√£o necess√°rios)
- qwen2.5:14b (~9GB)
- llama3:8b (~4.7GB)
- llama3.3:70b-instruct-q2_K (~26GB)
- qwen2.5-coder:32b-64k (~19GB)
- qwen2.5-coder:32b (~19GB)

**Espa√ßo libertado:** ~130-150GB

**Modelos locais mantidos:**
- qwen3-coder:30b ‚úÖ
- qwen3:32b
- llama3.3-64k:latest
- llama3.3:latest

## 22:00+ - Integra√ß√£o Grok 3
**Procedimento seguro via SSH:**
1. API key xAI guardada em tech/secrets/xai_key (chmod 600)
2. Provider xai configurado em openclaw.json
3. Modelo grok-3 adicionado (131k context)
4. Gateway reiniciado automaticamente

**Modelos FINAIS (ap√≥s limpeza):**
1. GLM-5:cloud (Ollama remoto) - Primary
2. Grok 3 (xAI) üÜï - Free + search integrado
3. Kimi 8k/32k/128k (Moonshot)
4. Gemini 2.5 Flash (Google AI)
5. qwen3-coder:30b (Local/fallback)

**DeepSeek R1 Free removido** (redundante)

## 22:00 - Monitor Zeus Alert
165 erros cr√≠ticos detetados (subiu de 74)
- Possivelmente relacionado com remo√ß√µes de modelos
- Recomenda√ß√£o: Verificar /home/clopes/.openclaw/logs/

## 23:00+ - Otimiza√ß√£o Final da Configura√ß√£o
**Carlos otimizou a configura√ß√£o de modelos.**

**Altera√ß√µes:**
1. Primary mudado: GLM-5 ‚Üí **Grok 3 (xai/grok-3)**
2. Fallbacks reorganizados:
   - Fallback 1: Gemini 2.5 Flash
   - Fallback 2: Kimi 32k (mudado de 8k para maior contexto)
   - Fallback 3: Gemini 1.5 Pro ‚Üí depois mudado para qwen3-coder:30b

**Testes de Modelos (23:06+):**
- ‚úÖ Gemini 2.5 Flash - Operacional
- ‚úÖ Gemini 1.5 Pro - Operacional
- ‚úÖ Grok 3 - Operacional

**Testes com alias tempor√°rio (23:11):**
- ‚úÖ Grok 3 - Respondeu com sucesso
- ‚ùå Kimi 32k - Sem output (problema de conex√£o)
- ‚ùå Gemini 2.5 Flash - Sem output (problema de conex√£o)

**DeepSeek R1 Free removido** (redundante, n√£o estava em uso)

## Status Final (23:13)
- ‚úÖ Sistema limpo e otimizado
- ‚úÖ **Primary model:** Grok 3 (xai/grok-3)
- ‚úÖ Modelos funcionais: Gemini 2.5/1.5, Kimi, Grok, GLM, Qwen3-local
- ‚úÖ Espa√ßo em disco libertado (~130-150GB)
- ‚úÖ Fallback chain: Gemini 2.5 ‚Üí Kimi 32k ‚Üí Qwen3
- ‚úÖ Testes de seguran√ßa: API keys guardadas com chmod 600
- ‚ö†Ô∏è Google AI key deve ser trocada URGENTEMENTE
- ‚ö†Ô∏è 165 erros cr√≠ticos a monitorizar (relacionados com remo√ß√µes de modelos)

## Modelos Finais Dispon√≠veis
1. **Grok 3** (Primary) - xAI, free com search
2. **Gemini 2.5 Flash** - Google AI, 1M context
3. **Gemini 1.5 Pro** - Google AI, 2M context
4. **Kimi 8k/32k/128k** - Moonshot
5. **GLM-5:cloud** - Alibaba/Ollama remoto
6. **Qwen3-Coder 30B** - Local/Ollama

## Configura√ß√£o Consolidada
- Heartbeat: ollama/qwen3-coder:30b (1h30m)
- Subagents: ollama/qwen3-coder:30b
- TTL context: 90m
- Max concurrent: 3 (normal), 4 (subagents)